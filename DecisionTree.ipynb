{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1:  What is a Decision Tree, and how does it work in the context of classification?**\n",
        "\n",
        "Ans = A Decision Tree is a supervised machine learning algorithm used to make decisions by recursively splitting data based on feature values, creating a tree-like structure that is both easy to interpret and visualize.\n",
        "\n",
        "Structure and Components\n",
        "\n",
        "The tree starts with a root node representing the full dataset.\n",
        "\n",
        "Each internal node corresponds to a decision based on an input feature.\n",
        "\n",
        "Branches denote the outcome of decisions (e.g., yes/no or specific ranges).\n",
        "\n",
        "Leaf nodes are terminal points that assign a class label or category to subset data.\n",
        "\n",
        "How Decision Trees Work in Classification\n",
        "\n",
        "Classification trees are used to predict categorical outcomes, such as “spam” or “not spam” for emails, by asking a series of questions based on feature values.\n",
        "\n",
        "The tree evaluates features using splitting criteria like Gini impurity or entropy, dividing the dataset into homogenous groups with regard to the target class.\n",
        "\n",
        "The algorithm continues splitting at each node until a stopping criterion is met, such as maximum tree depth, a minimum number of samples, or purity of leaf nodes (all samples in a node belong to the same class).\n",
        "\n",
        "Example in Classification\n",
        "\n",
        "For example, if the goal is to classify if a person is “fit” or “unfit”:\n",
        "The tree may first split by age, then by eating habits, and so on.\n",
        "\n",
        "At each split, a decision node asks a question, and each branch directs to a new question or outcome based on the answer.\n",
        "\n",
        "The process continues until the data cannot be split further, and the final classification is assigned at the leaf node"
      ],
      "metadata": {
        "id": "bZa6wclC0muz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2: Explain the concepts of Gini Impurity and Entropy as impurity measures.\n",
        "How do they impact the splits in a Decision Tree?**\n",
        "\n",
        "\n",
        "Gini Impurity\n",
        "\n",
        "It tells us: “How often would we be wrong if we randomly guessed the class of an item in this group?”\n",
        "\n",
        "If all items are from the same class → Gini = 0 (pure).\n",
        "\n",
        "If the items are mixed → Gini is higher.\n",
        "\n",
        "The decision tree will split in a way that reduces this wrong-guess chance.\n",
        "\n",
        "Entropy\n",
        "\n",
        "It comes from information theory and measures the disorder or uncertainty in a group.\n",
        "\n",
        "If a group has only one class → Entropy = 0 (no uncertainty).\n",
        "\n",
        "If a group has many classes evenly mixed → Entropy is higher (high uncertainty).\n",
        "\n",
        "The decision tree will split to reduce this uncertainty as much as possible.\n",
        "\n",
        "Impact on Splits\n",
        "\n",
        "Both Gini and Entropy help the tree decide where to split the data.\n",
        "\n",
        "The goal is always to make the groups purer (more of one class, less mixing).\n",
        "\n",
        "Difference:\n",
        "\n",
        "Gini is simpler and faster → often used by default.\n",
        "\n",
        "Entropy is more precise about how mixed the classes are but a bit slower.\n",
        "\n",
        "In practice, both usually give very similar decision trees."
      ],
      "metadata": {
        "id": "w2-5krROa3Fi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 3: What is the difference between Pre-Pruning and Post-Pruning in Decision\n",
        "Trees? Give one practical advantage of using each.**\n",
        "\n",
        "Ans = Difference Between Pre-Pruning and Post-Pruning\n",
        "Pre-Pruning (Early Stopping):\n",
        "This technique stops the growth of the decision tree early, before the tree becomes too complex. During training, splits are prevented if they do not improve some performance criterion (like information gain above a threshold, minimum required samples per leaf, or maximum tree depth). Pre-pruning keeps the tree small from the beginning and is efficient because unnecessary branches are never created.\n",
        "\n",
        "Post-Pruning (Reduced Error or Cost-Complexity Pruning):\n",
        "Here, the decision tree is first fully grown, which may result in a very complex structure. Then, the tree is simplified by removing branches that have little importance (i.e., do not improve accuracy on validation data). Post-pruning typically uses cross-validation or error-based strategies to determine which nodes can be pruned to strike a balance between model complexity and accuracy.\n",
        "\n",
        "Practical Advantage of Each\n",
        "Advantage of Pre-Pruning:\n",
        "It can significantly reduce computation time and memory usage, especially on large datasets, by avoiding the creation of unnecessary branches and resulting in simpler, faster, and more interpretable trees.\n",
        "\n",
        "Advantage of Post-Pruning:\n",
        "It often produces more accurate models because it allows the algorithm to find complex patterns first, then removes only the parts proven unnecessary, which improves generalization to unseen data by combatting overfitting more systematically.\n",
        "\n",
        "Both approaches serve to regularize decision trees—pre-pruning is proactive, while post-pruning is reactive, and their use depends on the dataset and modeling requirements.Pre-Pruning and Post-Pruning are two different strategies used to control the complexity of decision trees and prevent overfitting.\n"
      ],
      "metadata": {
        "id": "w-Vo_uzPod6F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 4: What is Information Gain in Decision Trees, and why is it important for\n",
        "choosing the best split?**\n",
        "\n",
        "Ans = What is Information Gain?\n",
        "\n",
        "Information Gain is a measure of how much “uncertainty” (impurity) is reduced in the data after splitting it based on a feature.\n",
        "\n",
        "In other words, it tells us how well a particular attribute separates the data into classes.\n",
        "\n",
        "It is calculated as the difference between the impurity of the parent node (before the split) and the weighted impurity of child nodes (after the split).\n",
        "\n",
        "Why is it important in Decision Trees?\n",
        "\n",
        "Decision trees work by choosing the best feature to split on at each step.\n",
        "\n",
        "The “best split” is the one that gives the highest Information Gain.\n",
        "\n",
        "A higher Information Gain means:\n",
        "\n",
        "The feature reduces uncertainty more.\n",
        "\n",
        "The child nodes are purer (contain mostly one class).\n",
        "\n",
        "The split helps the model make more accurate predictions."
      ],
      "metadata": {
        "id": "FnnECjOgo81G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: What are some common real-world applications of Decision Trees, and\n",
        "what are their main advantages and limitations?\n",
        "Answer:\n",
        "2\n",
        "Dataset Info:\n",
        "● Iris Dataset for classification tasks (sklearn.datasets.load_iris() or\n",
        "provided CSV).\n",
        "● Boston Housing Dataset for regression tasks\n",
        "(sklearn.datasets.load_boston() or provided CSV).\n",
        "\n",
        "Ans = Applications\n",
        "\n",
        "Healthcare – Diagnosing diseases based on symptoms and medical history.\n",
        "\n",
        "Finance – Credit scoring and loan approval by checking income, credit history, etc.\n",
        "\n",
        "Marketing & Sales – Customer segmentation and predicting customer churn.\n",
        "\n",
        "Retail – Product recommendation and demand forecasting.\n",
        "\n",
        "Manufacturing – Quality control and defect detection.\n",
        "\n",
        "Education – Predicting student performance and dropout risks.\n",
        "\n",
        "Advantages\n",
        "\n",
        "Easy to understand & interpret (even without deep ML knowledge).\n",
        "\n",
        "Handles both classification & regression tasks.\n",
        "\n",
        "No need for heavy data preprocessing (works with numerical & categorical data).\n",
        "\n",
        "Non-linear relationships can be captured.\n",
        "\n",
        "Fast and efficient on small-to-medium datasets.\n",
        "\n",
        "Limitations\n",
        "\n",
        "Prone to overfitting (creates very complex trees).\n",
        "\n",
        "Unstable – small changes in data can change the tree structure.\n",
        "\n",
        "Biased towards features with many categories.\n",
        "\n",
        "Less accurate compared to ensemble methods (Random Forest, XGBoost)."
      ],
      "metadata": {
        "id": "v5heMKJopYky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(criterion=\"entropy\", random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "print(\"Classification Accuracy on Iris dataset:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVBG89f3p8Ev",
        "outputId": "af160bb4-5a0f-4688-9a08-ca7b3b8499ca"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Accuracy on Iris dataset: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load dataset\n",
        "housing = fetch_california_housing()\n",
        "X, y = housing.data, housing.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Decision Tree Regressor\n",
        "reg = DecisionTreeRegressor(random_state=42)\n",
        "reg.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = reg.predict(X_test)\n",
        "\n",
        "# Evaluate with Mean Squared Error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Regression MSE on Housing dataset:\", mse)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lU90r_H5qFcJ",
        "outputId": "a511380f-f3ca-49a0-9bfd-19357d61e684"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Regression MSE on Housing dataset: 0.495235205629094\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 6:   Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "● Train a Decision Tree Classifier using the Gini criterion\n",
        "● Print the model’s accuracy and feature importances**"
      ],
      "metadata": {
        "id": "XMHK82KCqPTC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train Decision Tree Classifier with Gini criterion\n",
        "clf = DecisionTreeClassifier(criterion=\"gini\", random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Model accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Feature importances\n",
        "print(\"Feature Importances:\", clf.feature_importances_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-YSTa2Qpqd4c",
        "outputId": "18372236-4efe-4284-94f6-4c8aab2e6b2d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n",
            "Feature Importances: [0.         0.01667014 0.90614339 0.07718647]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 7:  Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "● Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to\n",
        "a fully-grown tree.**"
      ],
      "metadata": {
        "id": "n0jGyPpjqsnb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split into train & test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train fully-grown decision tree (no max_depth limit)\n",
        "clf_full = DecisionTreeClassifier(random_state=42)\n",
        "clf_full.fit(X_train, y_train)\n",
        "y_pred_full = clf_full.predict(X_test)\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "# Train decision tree with max_depth=3\n",
        "clf_limited = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "clf_limited.fit(X_train, y_train)\n",
        "y_pred_limited = clf_limited.predict(X_test)\n",
        "accuracy_limited = accuracy_score(y_test, y_pred_limited)\n",
        "\n",
        "# Print results\n",
        "print(\"Accuracy of Fully-grown Decision Tree:\", accuracy_full)\n",
        "print(\"Accuracy of Decision Tree with max_depth=3:\", accuracy_limited)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "366SAlOZqyve",
        "outputId": "85c923e4-2947-408a-9864-144c6a48104c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Fully-grown Decision Tree: 1.0\n",
            "Accuracy of Decision Tree with max_depth=3: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 8: Write a Python program to:\n",
        "● Load the Boston Housing Dataset\n",
        "● Train a Decision Tree Regressor\n",
        "● Print the Mean Squared Error (MSE) and feature importances**"
      ],
      "metadata": {
        "id": "uLnPlCiAq5yw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load California Housing dataset (replacement for Boston Housing)\n",
        "housing = fetch_california_housing()\n",
        "X, y = housing.data, housing.target\n",
        "\n",
        "# Split into train & test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train Decision Tree Regressor\n",
        "reg = DecisionTreeRegressor(random_state=42)\n",
        "reg.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = reg.predict(X_test)\n",
        "\n",
        "# Mean Squared Error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Mean Squared Error (MSE):\", mse)\n",
        "\n",
        "# Feature importances\n",
        "print(\"\\nFeature Importances:\")\n",
        "for feature, importance in zip(housing.feature_names, reg.feature_importances_):\n",
        "    print(f\"{feature}: {importance:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J4VCydSZrUth",
        "outputId": "854eb780-f828-4301-907f-e3e1f0be0d27"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (MSE): 0.495235205629094\n",
            "\n",
            "Feature Importances:\n",
            "MedInc: 0.5285\n",
            "HouseAge: 0.0519\n",
            "AveRooms: 0.0530\n",
            "AveBedrms: 0.0287\n",
            "Population: 0.0305\n",
            "AveOccup: 0.1308\n",
            "Latitude: 0.0937\n",
            "Longitude: 0.0829\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 9: Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "● Tune the Decision Tree’s max_depth and min_samples_split using\n",
        "GridSearchCV\n",
        "● Print the best parameters and the resulting model accuracy**"
      ],
      "metadata": {
        "id": "K7oMnHN8rdok"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split into train & test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Define Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Define parameter grid for tuning\n",
        "param_grid = {\n",
        "    \"max_depth\": [2, 3, 4, 5, None],   # try shallow to deep trees\n",
        "    \"min_samples_split\": [2, 3, 4, 5, 10]  # min samples required to split\n",
        "}\n",
        "\n",
        "# GridSearchCV for hyperparameter tuning\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=clf,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,            # 5-fold cross-validation\n",
        "    scoring=\"accuracy\"\n",
        ")\n",
        "\n",
        "# Fit grid search\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "\n",
        "# Best model\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Evaluate accuracy on test set\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy with Best Parameters:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVCEFpSzrk8N",
        "outputId": "50f15c81-3f13-45ac-ba38-19ba3edeb4ae"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 4, 'min_samples_split': 2}\n",
            "Accuracy with Best Parameters: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 10: Imagine you’re working as a data scientist for a healthcare company that\n",
        "wants to predict whether a patient has a certain disease. You have a large dataset with\n",
        "mixed data types and some missing values.\n",
        "Explain the step-by-step process you would follow to:\n",
        "● Handle the missing values\n",
        "● Encode the categorical features\n",
        "● Train a Decision Tree model\n",
        "● Tune its hyperparameters\n",
        "● Evaluate its performance\n",
        "And describe what business value this model could provide in the real-world\n",
        "setting.**"
      ],
      "metadata": {
        "id": "LqRKxnJbrszW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step-by-Step Process\n",
        "\n",
        "**1. Handle Missing Values**\n",
        "\n",
        "Check missingness: Use .isnull().sum() to see which features have missing values.\n",
        "\n",
        "Strategy:\n",
        "\n",
        "For numerical features → fill with mean/median (e.g., blood pressure, age).\n",
        "\n",
        "For categorical features → fill with mode (most frequent category, e.g., gender = Male/Female).\n",
        "\n",
        "If a feature has too many missing values, consider dropping it if not critical.\n",
        "\n",
        "2. Encode the Categorical Features\n",
        "\n",
        "Decision Trees can’t handle text directly.\n",
        "\n",
        "Options:\n",
        "\n",
        "One-Hot Encoding (good for features like “smoker: Yes/No”).\n",
        "\n",
        "Label Encoding (if categories have natural order, e.g., “Stage 1, Stage 2, Stage 3”).\n",
        "\n",
        "3. Train a Decision Tree Model\n",
        "\n",
        "Split dataset into train (80%) and test (20%).\n",
        "\n",
        "Initialize a Decision Tree Classifier:\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "model = DecisionTreeClassifier(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "4. Tune Hyperparameters\n",
        "\n",
        "Use GridSearchCV to find the best parameters:\n",
        "\n",
        "max_depth (controls tree depth).\n",
        "\n",
        "min_samples_split (minimum samples to split a node).\n",
        "\n",
        "criterion (e.g., \"gini\" or \"entropy\").\n",
        "\n",
        "Example:\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "param_grid = {\n",
        "    \"max_depth\": [3, 5, 7, None],\n",
        "    \"min_samples_split\": [2, 5, 10],\n",
        "    \"criterion\": [\"gini\", \"entropy\"]\n",
        "}\n",
        "grid = GridSearchCV(model, param_grid, cv=5, scoring=\"accuracy\")\n",
        "grid.fit(X_train, y_train)\n",
        "best_model = grid.best_estimator_\n",
        "\n",
        "5. Evaluate Performance\n",
        "\n",
        "Predict on the test set:\n",
        "\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "y_pred = best_model.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "\n",
        "Use metrics:\n",
        "\n",
        "Accuracy → overall performance.\n",
        "\n",
        "Precision/Recall/F1-score → very important in healthcare (e.g., false negatives can be dangerous).\n",
        "\n",
        "ROC-AUC → measures probability ranking quality.\n",
        "\n",
        "Business Value in Real-World Healthcare\n",
        "\n",
        "Faster diagnosis: Helps doctors identify high-risk patients quickly.\n",
        "\n",
        "Resource optimization: Prioritize tests for patients likely to have the disease.\n",
        "\n",
        "Personalized treatment: Different features (like age, lifestyle, genetics) help tailor care plans.\n",
        "\n",
        "Cost savings: Reduces unnecessary tests for low-risk patients.\n",
        "\n",
        "Improved patient outcomes: Early detection leads to better treatment success."
      ],
      "metadata": {
        "id": "rmDCy4t_srWJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# -----------------------\n",
        "# 1. Create Example Dataset (simulated healthcare data)\n",
        "# -----------------------\n",
        "data = {\n",
        "    \"age\": [25, 47, 52, np.nan, 36, 29, 65, 41, np.nan, 55],\n",
        "    \"gender\": [\"Male\", \"Female\", \"Female\", \"Male\", \"Male\", np.nan, \"Female\", \"Male\", \"Female\", \"Male\"],\n",
        "    \"blood_pressure\": [120, 140, 130, 135, np.nan, 128, 150, 145, 138, np.nan],\n",
        "    \"smoker\": [\"Yes\", \"No\", \"No\", \"Yes\", \"No\", \"Yes\", \"No\", \"No\", \"Yes\", \"No\"],\n",
        "    \"disease\": [0, 1, 1, 0, 0, 1, 1, 0, 1, 1]  # Target (0 = No Disease, 1 = Disease)\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Features and Target\n",
        "X = df.drop(\"disease\", axis=1)\n",
        "y = df[\"disease\"]\n",
        "\n",
        "# -----------------------\n",
        "# 2. Preprocessing Pipeline\n",
        "# -----------------------\n",
        "\n",
        "# Separate categorical and numerical features\n",
        "categorical_features = [\"gender\", \"smoker\"]\n",
        "numerical_features = [\"age\", \"blood_pressure\"]\n",
        "\n",
        "# Transformers\n",
        "categorical_transformer = OneHotEncoder(handle_unknown=\"ignore\")\n",
        "# Numerical missing values will be handled inside Decision Tree (or we can fillna)\n",
        "\n",
        "# Preprocessor\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"cat\", categorical_transformer, categorical_features),\n",
        "        (\"num\", \"passthrough\", numerical_features)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# -----------------------\n",
        "# 3. Build Pipeline with Decision Tree\n",
        "# -----------------------\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "pipeline = Pipeline(steps=[(\"preprocessor\", preprocessor),\n",
        "                           (\"classifier\", dt)])\n",
        "\n",
        "# ---\n"
      ],
      "metadata": {
        "id": "BdMBkGtDtHc5"
      },
      "execution_count": 9,
      "outputs": []
    }
  ]
}